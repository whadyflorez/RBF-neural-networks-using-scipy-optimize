# -*- coding: utf-8 -*-
"""ejemplo_modelo_torch_chatgpt_con_funciones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10HGBzMBfqk13ETIDuE1CjtpjeirsLOLh
"""



"""# Como definir un problema en pytorch sin usar classes"""

import torch
import torch.optim as optim

# Función para definir el modelo
def simple_model(x, weights, bias):
    return torch.matmul(x, weights) + bias

# Función de pérdida personalizada
def custom_loss(y_pred, y_true):
    return torch.mean((y_pred - y_true) ** 2)

# Función para calcular los gradientes manualmente de forma analítica
def calculate_gradients(x, y, weights, bias):
    with torch.no_grad():
        gradient = torch.mean(2 * (simple_model(x, weights, bias) - y) * x, dim=0)
        bias_gradient = torch.mean(2 * (simple_model(x, weights, bias) - y))

    return gradient, bias_gradient

# Inicializar los parámetros del modelo
weights = torch.randn(1, 1, requires_grad=True)
bias = torch.randn(1, requires_grad=True)

# Datos de ejemplo
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0], [10.0]])

# Definir el optimizador
#optimizer = optim.SGD([weights, bias], lr=0.1)
optimizer = optim.SGD([weights, bias], lr=0.01, momentum=0.9)

# Ciclo de entrenamiento
epochs = 200
batch_size = 3
num_batches = len(x_train) // batch_size

for epoch in range(epochs):
    # Iterar sobre los lotes de datos
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = start_idx + batch_size

        # Obtener el lote actual
        x_batch = x_train[start_idx:end_idx]
        y_batch = y_train[start_idx:end_idx]

        # Paso de adelante: Calcular la predicción y la pérdida
        y_pred = simple_model(x_batch, weights, bias)
        loss = custom_loss(y_pred, y_batch)

        # Calcular los gradientes manualmente de forma analítica
        weight_gradient, bias_gradient = calculate_gradients(x_batch, y_batch, weights, bias)

        # Actualizar los parámetros del modelo utilizando el optimizador
        optimizer.zero_grad()
        weights.grad = weight_gradient.reshape_as(weights)
        bias.grad = bias_gradient.reshape_as(bias)
        optimizer.step()

    # Imprimir la pérdida
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')

# Evaluar el modelo entrenado
with torch.no_grad():
    print("Predicciones después del entrenamiento:")
    y_pred = simple_model(x_train, weights, bias)
    for i, (predicted, true) in enumerate(zip(y_pred, y_train)):
        print(f'Entrada: {x_train[i].item()}, Predicción: {predicted.item()}, Valor real: {true.item()}')

"""# Como usar LBFGS full batch"""

import torch
import torch.optim as optim

# Función para definir el modelo
def simple_model(x, weights, bias):
    return torch.matmul(x, weights) + bias

# Función de pérdida personalizada
def custom_loss(y_pred, y_true):
    return torch.mean((y_pred - y_true) ** 2)

# Función para calcular los gradientes manualmente de forma analítica
def calculate_gradients(x, y, weights, bias):
    with torch.no_grad():
        gradient = torch.mean(2 * (simple_model(x, weights, bias) - y) * x, dim=0)
        bias_gradient = torch.mean(2 * (simple_model(x, weights, bias) - y))

    return gradient, bias_gradient

# Inicializar los parámetros del modelo
weights = torch.randn(1, 1, requires_grad=True)
bias = torch.randn(1, requires_grad=True)

# Datos de ejemplo
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0], [10.0]])

# Definir el optimizador LBFGS
optimizer = optim.LBFGS([weights, bias], lr=0.1)

# Función para realizar un paso de optimización con LBFGS
def closure():
    optimizer.zero_grad()
    y_pred = simple_model(x_train, weights, bias)
    loss = custom_loss(y_pred, y_train)

    # Calcular los gradientes manualmente
    weight_gradient, bias_gradient = calculate_gradients(x_train, y_train, weights, bias)

    # Asignar los gradientes calculados manualmente
    weights.grad = weight_gradient.reshape_as(weights)
    bias.grad = bias_gradient.reshape_as(bias)

    return loss

# Ciclo de entrenamiento
epochs = 100

for epoch in range(epochs):
    optimizer.step(closure)

    # Imprimir la pérdida
    if (epoch+1) % 10 == 0:
        with torch.no_grad():
            loss = closure()
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')

# Evaluar el modelo entrenado
with torch.no_grad():
    print("Predicciones después del entrenamiento:")
    y_pred = simple_model(x_train, weights, bias)
    for i, (predicted, true) in enumerate(zip(y_pred, y_train)):
        print(f'Entrada: {x_train[i].item()}, Predicción: {predicted.item()}, Valor real: {true.item()}')

"""# Como usar LBFGS en batches"""

import torch
import torch.optim as optim

# Función para definir el modelo
def simple_model(x, weights, bias):
    return torch.matmul(x, weights) + bias

# Función de pérdida personalizada
def custom_loss(y_pred, y_true):
    return torch.mean((y_pred - y_true) ** 2)

# Función para calcular los gradientes manualmente de forma analítica
def calculate_gradients(x, y, weights, bias):
    with torch.no_grad():
        gradient = torch.mean(2 * (simple_model(x, weights, bias) - y) * x, dim=0)
        bias_gradient = torch.mean(2 * (simple_model(x, weights, bias) - y))

    return gradient, bias_gradient

# Inicializar los parámetros del modelo
weights = torch.randn(1, 1, requires_grad=True)
bias = torch.randn(1, requires_grad=True)

# Datos de ejemplo
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0], [10.0]])

# Definir el optimizador LBFGS
optimizer = optim.LBFGS([weights, bias], lr=0.01)

# Función para realizar un paso de optimización con LBFGS
def closure():
    optimizer.zero_grad()
    total_loss = 0
    for i in range(0, len(x_train), batch_size):
        x_batch = x_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]

        y_pred = simple_model(x_batch, weights, bias)
        loss = custom_loss(y_pred, y_batch)
        total_loss += loss.item()

        # Calcular los gradientes manualmente
        weight_gradient, bias_gradient = calculate_gradients(x_batch, y_batch, weights, bias)

        # Asignar los gradientes calculados manualmente
        weights.grad = weight_gradient.reshape_as(weights)
        bias.grad = bias_gradient.reshape_as(bias)

    # Retornar el total de la pérdida para el cálculo de la media
    return total_loss / len(x_train)

# Ciclo de entrenamiento
epochs = 100
batch_size = 3

for epoch in range(epochs):
    optimizer.step(closure)

    # Imprimir la pérdida
    if (epoch+1) % 10 == 0:
        with torch.no_grad():
            loss = closure()
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss}')

# Evaluar el modelo entrenado
with torch.no_grad():
    print("Predicciones después del entrenamiento:")
    y_pred = simple_model(x_train, weights, bias)
    for i, (predicted, true) in enumerate(zip(y_pred, y_train)):
        print(f'Entrada: {x_train[i].item()}, Predicción: {predicted.item()}, Valor real: {true.item()}')