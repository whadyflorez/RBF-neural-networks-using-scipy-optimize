# RBF-neural-networks-using-scipy-optimize
RBF neural networks using scipy optimize
ML does not depend on pytorch neither on TF or anything similar. ML is a series of nunmerical and computational techniques based on data to solve many problems of different kinds. It should nnot be considered a separate science of its own.
RBF networks requiere special training algorithms that are not always the same as deep NN.

https://datascience.stackexchange.com/questions/44324/how-to-get-out-of-local-minimums-on-stochastic-gradient-descent
https://arxiv.org/abs/2208.00441
https://arxiv.org/abs/2105.14694
https://maths-people.anu.edu.au/~brent/pub/pub011.html
https://math.stackexchange.com/questions/2349026/why-is-the-approximation-of-hessian-jtj-reasonable
https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm
https://www.mdpi.com/2076-3417/10/6/2036
https://arxiv.org/pdf/1905.06738.pdf
 
https://doi.org/10.3390/electronics9111809
https://link.springer.com/article/10.1007/s10898-022-01205-4
https://www.sciencedirect.com/science/article/pii/S0893608020303579

https://pypi.org/project/ncg-optimizer/#description
https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html

Why should you have special training of RBF NN? "In this article, we describe a modern gradient-based RBFN implementation based on the same computational machinery that is used in modern deep learning. While gradient-based methods for training RBFNâ€™s have been criticized (Chen, Cowan, & Grant, 1991) because of local optima, tailored training methods have been used in the past with good results"

@INPROCEEDINGS{614174,
  author={Karayiannis, N.B.},
  booktitle={Proceedings of International Conference on Neural Networks (ICNN'97)}, 
  title={Gradient descent learning of radial basis neural networks}, 
  year={1997},
  volume={3},
  number={},
  pages={1815-1820 vol.3},
  keywords={Neural networks;Radial basis function networks;Prototypes;Clustering algorithms;Surface fitting;Vector quantization;Training data;Computer networks;Sensitivity analysis;Multidimensional systems},
  doi={10.1109/ICNN.1997.614174}}
"This paper presents an axiomatic approach for building RBF neural networks and also proposes a supervised learning algorithm based on gradient descent for their training. This approach results in a broad variety of admissible RBF models, including those employing Gaussian radial basis functions. The form of the radial basis functions is determined by a generator function. A sensitivity analysis explains the failure of gradient descent learning on RBF networks with Gaussian radial basis functions, which are generated by an exponential generator function. The same analysis verifies that RBF networks generated by a linear generator function are much more suitable for gradient descent learning. Experiments involving such RBF networks indicate that the proposed gradient descent algorithm guarantees fast learning and very satisfactory generalization ability."
