# -*- coding: utf-8 -*-
"""ejemplo_modelo_torch_chatgpt_con_funciones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10HGBzMBfqk13ETIDuE1CjtpjeirsLOLh
"""



"""# Como definir un problema en pytorch sin usar classes"""

import torch
import torch.optim as optim
import numpy as np
import pytorch_soo as soo
from pytorch_soo.line_search_spec import LineSearchSpec


nd=10

c=np.sqrt(8.0)
def rbf(x,xs):
    r=torch.norm(x-xs)
    y=torch.sqrt(r**2+c**2)
    return y

def laplace_rbf(x,xs):
    r=torch.norm(x-xs)
    y=(2.0*c**2+r**2)/rbf(x,xs)**3
    return y    

def drbf(x,xs):
    dfdx=(x[0]-xs[0])/rbf(x,xs)
    dfdy=(x[1]-xs[1])/rbf(x,xs)   
    return dfdx,dfdy

z=torch.zeros(nd)
A=torch.zeros(nd,nd)

# A[0,:]=torch.tensor([1.0,2.0,-5.0,1.3])
# A[1,:]=torch.tensor([0.0,-1.7,3.0,5.3])
# A[2,:]=torch.tensor([10.0,-4.0,3.5,6.8])
# A[3,:]=torch.tensor([5.0,1.0,4.0,1.0])

# z[:]=torch.tensor([1.0,2.0,3.0,4.0])

     
#AT=torch.t(A)
torch.manual_seed(1000)    

A=torch.rand(nd,nd)
z=torch.rand(nd) 

        
def loss(p,*args):
    ibatch=args[0]
    ybatch=0.0
    for i in ibatch:
        ybatch+=0.5*(torch.dot(A[i,:],p)-z[i])**2
    return ybatch

def grad(p,*args):
   ibatch=args[0]
   with torch.no_grad():
       gradbatch = torch.zeros_like(p)
       for i in ibatch:
           gradbatch+=(torch.dot(A[i,:],p)-z[i])*A[i,:]
   return gradbatch   

# Inicializar los parámetros del modelo
a,b=-0.5,0.5
p = torch.rand(nd)
p = (b-a)*p+a
#p.requires_grad=True

#define scales
#s=torch.abs(p)
#s=torch.where(s<=1.0e-6,1.0,s) 
s=torch.ones_like(p)

# Definir el optimizador
optimizer = optim.LBFGS([p],lr=1.0,history_size=100,max_iter=100,max_eval=400,\
                        line_search_fn='strong_wolfe' )
#optimizer = soo.HFCR_Newton([p],max_cr=50,max_newton=10)
#optimizer = optim.NAdam([p],lr=0.1)

# Función para realizar un paso de optimización con LBFGS
def closure():
    optimizer.zero_grad()
    loss_val=loss(p*s,arr.tolist())
    p_grad=grad(p*s,ibatch)*s
    p.grad = p_grad
    return loss_val

# Ciclo de entrenamiento
max_iter = 100
batch_size =int((nd))

arr=torch.randperm(nd)
ibatch=arr[0:batch_size]

for i in range(max_iter):
    optimizer.step(closure)
    arr=torch.randperm(nd)
    ibatch=arr[0:batch_size]
    loss_val=closure()  
    print(f'iteration {i}, Loss: {loss_val.item()}')

p_exact=torch.linalg.solve(A,z)
    
    





